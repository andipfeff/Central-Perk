{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69f7e612",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7debe3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ddb03f",
   "metadata": {},
   "source": [
    "# Scrape Friends Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ad756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe to hold script\n",
    "script_df = pd.DataFrame(columns = ['line','season','episode','line_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ca21f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize chromediriver\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()} \n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7102ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# navigate to script website\n",
    "url = 'https://transcripts.foreverdreaming.org/viewforum.php?f=845'\n",
    "browser.visit(url)\n",
    "\n",
    "# parse HTML of first landing page\n",
    "html = browser.html\n",
    "bsoup = soup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaf8bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,10):\n",
    "    \n",
    "    if i == 1:\n",
    "        \n",
    "        # Find all episode title tags\n",
    "        seas_ep = bsoup.find_all('h3')\n",
    "        \n",
    "        # loop through episode links on page.\n",
    "        for j in range(1,(len(seas_ep))):\n",
    "    \n",
    "            # Identify episode number and title text\n",
    "            link_text = seas_ep[j].a.get_text()\n",
    "            \n",
    "            # navigate to page and parse the new page \n",
    "            browser.links.find_by_partial_text(link_text).click()\n",
    "            \n",
    "            # parse HTML for episode and initialize BeautifulSoup\n",
    "            html = browser.html\n",
    "            bsoup = soup(html, 'html.parser')\n",
    "            \n",
    "            # Identify div that holds scripts\n",
    "            scripts = bsoup.find('div', class_='postbody')\n",
    "            \n",
    "            # Grab all 'p' tags within script div\n",
    "            all_lines = scripts.find_all('p')\n",
    "            \n",
    "            # print episode/season to terminal\n",
    "            print(f'gathering scripts from \"{link_text}\"')\n",
    "    \n",
    "            # loop through individual lines and add to dataframe\n",
    "            for line in all_lines:\n",
    "        \n",
    "                try:\n",
    "                    # Grab script line\n",
    "                    line = line.get_text()\n",
    "            \n",
    "                    # Get Season & episode\n",
    "                    seas=int(link_text[:2])\n",
    "                    ep=int(link_text[3:5])\n",
    "            \n",
    "                    # combine Season & Episode to create line ID\n",
    "                    lineID= int((seas*100)+(ep))\n",
    "                    #print(f'{lineID} {line}')\n",
    "            \n",
    "                    # Create a temp dataframe to hold new line\n",
    "                    temp_df = pd.DataFrame([[line,seas,ep,lineID]],\n",
    "                                           columns =['line','season','episode','line_ID'] )\n",
    "            \n",
    "                    # add new line to dataframe\n",
    "                    script_df = pd.concat([script_df,temp_df], ignore_index=True, axis=0)\n",
    "            \n",
    "            \n",
    "                except:\n",
    "                    print('skipped - error')\n",
    "                    pass\n",
    "    \n",
    "            browser.back()\n",
    "    \n",
    "        \n",
    "    else:\n",
    "        # navigate to the next page\n",
    "        page_numbers = browser.links.find_by_text(i)\n",
    "        page_numbers.click()\n",
    "        \n",
    "        # parse HTML of first landing page\n",
    "        html = browser.html\n",
    "        bsoup = soup(html, 'html.parser')\n",
    "        \n",
    "        # Find all episode title tags\n",
    "        seas_ep = bsoup.find_all('h3')\n",
    "        \n",
    "        # loop through episode links on page.\n",
    "        for j in range(1,(len(seas_ep))):\n",
    "    \n",
    "            # Identify episode number and title text\n",
    "            link_text = seas_ep[j].a.get_text()\n",
    "            \n",
    "            # navigate to page and parse the new page \n",
    "            browser.links.find_by_partial_text(link_text).click()\n",
    "            \n",
    "            # parse HTML for episode and initialize BeautifulSoup\n",
    "            html = browser.html\n",
    "            bsoup = soup(html, 'html.parser')\n",
    "            \n",
    "            # Identify div that holds scripts\n",
    "            scripts = bsoup.find('div', class_='postbody')\n",
    "            \n",
    "            # Grab all 'p' tags within script div\n",
    "            all_lines = scripts.find_all('p')\n",
    "            \n",
    "            # print episode/season to terminal\n",
    "            print(f'gathering scripts from \"{link_text}\"')\n",
    "    \n",
    "            # loop through individual lines and add to dataframe\n",
    "            for line in all_lines:\n",
    "        \n",
    "                try:\n",
    "                    # Grab script line\n",
    "                    line = line.get_text()\n",
    "            \n",
    "                    # Get Season & episode\n",
    "                    seas=int(link_text[:2])\n",
    "                    ep=int(link_text[3:5])\n",
    "            \n",
    "                    # combine Season & Episode to create line ID\n",
    "                    lineID= int((seas*100)+(ep))\n",
    "                    #print(f'{lineID} {line}')\n",
    "            \n",
    "                    # Create a temp dataframe to hold new line\n",
    "                    temp_df = pd.DataFrame([[line,seas,ep,lineID]],\n",
    "                                           columns =['line','season','episode','line_ID'] )\n",
    "            \n",
    "                    # add new line to dataframe\n",
    "                    script_df = pd.concat([script_df,temp_df], ignore_index=True, axis=0)\n",
    "            \n",
    "            \n",
    "                except:\n",
    "                    print('skipped - error')\n",
    "                    pass\n",
    "    \n",
    "            browser.back()\n",
    "    \n",
    "\n",
    "            \n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f84d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "            # Optional delay for loading the page\n",
    "            browser.is_element_present_by_css('div.list_text', wait_time=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d93318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview dataframe\n",
    "script_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2692e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe to a csv\n",
    "script_df.to_csv(r\"../Resources/data/Friends_Episodes_Transcripts.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
